# 《Text and Code Embeddings by Contrastive Pre-Training》

## 摘要

文本嵌入（Embeddings）在许多应用中都是有用的功能，例如语义搜索和计算文本相似性。先前的工作通常训练针对不同用例定制的模型，这些用例在数据集选择、训练目标和模型架构方面有所不同。在这项工作中，我们表明，在规模上对无监督数据进行对比预训练可以产生文本和代码的高质量向量表示。在线性探测分类中获得新的最先进结果的无监督文本嵌入也显示出令人印象深刻的语义搜索能力，有时甚至与微调模型相比具有竞争力。在7个任务的线性探针分类精度平均值上，我们的最佳无监督模型分别比以前的最佳无监管和有监督文本嵌入模型实现了4%和1.8%的相对改进。在大规模语义搜索上评估相同的文本嵌入时，与之前在MSMARCO、Natural Questions和TriviaQA基准上的最佳无监督方法相比，分别获得了23.4%、14.7%和10.6%的相对改进。与文本嵌入类似，我们在（文本，代码）对上训练代码嵌入模型，在代码搜索方面获得了20.8%的相对改进。

## 模型细节

模型是以一对一对的数据为对比目标进行训练的。在本节中，我们将详细介绍模型体系结构和训练目标。

训练集由成对样本组成，$(\{(x_i,y_i)\})_{i=1}^N)$，其中$(x_i,y_i)$为正例对，表明$x_i$和$y_i$在语义上相似或上下文相关。

给定训练对$(x,y)$，使用Transformer编码器E独立处理x和y。
编码器将输入映射到embedding。在输入序列的开始和结束处分别插入两个特殊的标记分隔符[SOS]和[EOS]。与特殊令牌[EOS]相对应的最后一层的隐藏状态被认为是输入序列的embedding。

